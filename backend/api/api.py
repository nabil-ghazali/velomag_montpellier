from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import os
import pandas as pd
from dotenv import load_dotenv
from backend.data.schemas import Database

# 1. Configuration
load_dotenv()
app = FastAPI(title="VÃ©loMag API")

# Autoriser Streamlit (qui tourne sur un autre port) Ã  parler Ã  l'API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 2. Connexion Base de DonnÃ©es
try:
    USER = os.getenv("user")
    PASSWORD = os.getenv("password")
    HOST = os.getenv("host")
    PORT = os.getenv("port")
    DBNAME = os.getenv("dbname")
    DATABASE_URL = f"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DBNAME}?sslmode=require"
    db = Database(DATABASE_URL)
except Exception as e:
    print(f" Erreur DB connection: {e}")

# --- ROUTES ---

@app.get("/")
def root():
    return {"message": "API VÃ©loMag est en ligne ! ðŸš²"}

@app.get("/counters")
def get_list_counters():
    """Retourne la liste unique des compteurs disponibles."""
    try:
        # On lit la table model_data car c'est elle qui contient les prÃ©dictions prÃªtes
        df = db.pull_data("model_data")
        if df.empty:
            return {"counters": []}
        
        unique_ids = df['counter_id'].unique().tolist()
        return {"counters": unique_ids}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
# Cette version est trop lourde :C'est trÃ¨s clair. Oublions le monitoring.

# Voici l'explication technique : le problÃ¨me principal est la mÃ©thode "Tout tÃ©lÃ©charger, puis filtrer". Actuellement, quand un utilisateur demande l'historique d'un seul compteur, voici ce qui se passe :

# - Python appelle la Base de DonnÃ©es (PostgreSQL) et PostgreSQL envoie TOUTE la table velo_clean (800 000+ lignes, peut-Ãªtre 100 Mo ou plus) via le rÃ©seau vers Azure Web App.

# - Pandas charge ces 100 Mo dans la RAM, ensuite Pandas filtre pour ne garder que les 5 000 lignes du compteur demandÃ©.

# - Python jette les 795 000 autres lignes Ã  la poubelle.

# @app.get("/history/{counter_id}")
# def get_history(counter_id: str):
#     """Retourne l'historique rÃ©el pour un compteur."""
#     try:
#         df = db.pull_data("velo_clean")
#         # Filtrage
#         df = df[df['counter_id'] == counter_id]
#         df = df[['datetime', 'intensity']]         # on ne renvoie que les colonnes utiles
#         df = df.rename(columns={'intensity': 'count'})
#         # Conversion JSON
#         return df.to_dict(orient="records")
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))

@app.get("/history/{counter_id}")
def get_history(counter_id: str):
    # 1. On Ã©crit une requÃªte SQL ciblÃ©e
    # On ne demande QUE les colonnes utiles et QUE les lignes du compteur
    query = f"""
        SELECT datetime, intensity 
        FROM velo_clean 
        WHERE counter_id = '{counter_id}'
    """
    
    # 2. On exÃ©cute via le moteur SQLAlchemy dÃ©jÃ  prÃ©sent dans ton objet db
    with db.engine.connect() as conn:
        df = pd.read_sql(query, conn)
        
    # 3. Plus besoin de filtrer, c'est dÃ©jÃ  fait !
    df = df.rename(columns={'intensity': 'count'})
    
    # Petite astuce : convertir les dates en string pour Ã©viter les erreurs JSON
    df['datetime'] = df['datetime'].astype(str)
    
    return df.to_dict(orient="records")

# # Version lourde
# @app.get("/prediction/{counter_id}")
# def get_prediction(counter_id: str):
#     """Retourne les prÃ©dictions (J+1) pour un compteur."""
#     try:
#         df = db.pull_data("model_data")
#         df = df[df['counter_id'] == counter_id]
#         df = df[['datetime', 'predicted_values']]
#         df = df.rename(columns={'predicted_values': 'count'})
#         return df.to_dict(orient="records")
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))

@app.get("/prediction/{counter_id}")
def get_prediction(counter_id: str):
    """Retourne les prÃ©dictions (J+1) pour un compteur de faÃ§on optimisÃ©e."""
    try:
        # OPTIMISATION SQL :
        # 1. On filtre sur l'ID directement dans la base.
        # 2. On filtre sur la date (>= aujourd'hui) pour ne pas renvoyer de vieilles prÃ©dictions.
        query = f"""
            SELECT datetime, predicted_values 
            FROM model_data 
            WHERE counter_id = '{counter_id}'
            AND datetime >= CURRENT_DATE
            ORDER BY datetime ASC
        """
        
        with db.engine.connect() as conn:
            df = pd.read_sql(query, conn)
            
        # Si aucune prÃ©diction n'est trouvÃ©e (ex: mauvais ID)
        if df.empty:
            return []

        # Nettoyage et Renommage
        df = df.rename(columns={'predicted_values': 'count'})
        
        # Conversion Date pour JSON
        df['datetime'] = df['datetime'].astype(str)
        
        return df.to_dict(orient="records")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Cette version est lourde aussi : on rÃ©cupÃ¨res aussi tout model_data (lourd) et tout velo_clean pour avoir les positions GPS.
# @app.get("/map-data")
# def get_map_data():
#     """
#     Route spÃ©ciale pour la carte : 
#     RÃ©cupÃ¨re toutes les prÃ©dictions et joint les coordonnÃ©es GPS.
#     """
#     try:
#         # 1. RÃ©cupÃ©rer toutes les prÃ©dictions
#         df_preds = db.pull_data("model_data")
        
#         # 2. RÃ©cupÃ©rer les positions GPS (depuis velo_clean ou une table counters)
#         # On fait une requÃªte SQL pour avoir une seule ligne par compteur avec lat/lon
#         query = "SELECT DISTINCT ON (counter_id) counter_id, lat, lon FROM velo_clean"
#         with db.engine.connect() as conn:
#             df_locs = pd.read_sql(query, conn)
            
#         # 3. Fusionner (Join)
#         # On ajoute lat/lon aux prÃ©dictions
#         df_merged = pd.merge(df_preds, df_locs, on="counter_id", how="left")
        
#         # 4. Nettoyage pour le frontend
#         # On renomme pour coller Ã  votre code frontend
#         df_merged = df_merged.rename(columns={
#             'predicted_values': 'predicted_intensity',
#             'datetime': 'date'
#         })
        
#         # Astuce : Comme on n'a pas stockÃ© la tempÃ©rature dans model_data, 
#         # on met une valeur par dÃ©faut pour ne pas faire planter votre interface.
#         df_merged['temperature_2m'] = 15.0 
        
#         return df_merged.to_dict(orient="records")
        
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))


#premiÃ¨re version optimisÃ©
@app.get("/map-data")
def get_map_data():
    try:
        # 1. On rÃ©cupÃ¨re les prÃ©dictions (J+1 seulement)
        # Supposons que tu veux les prÃ©dictions de demain.
        # Si model_data ne contient QUE les prÃ©dictions futures, pull_data est OK.
        # Sinon, il faut filtrer par date en SQL.
        df_preds = db.pull_data("model_data")

        # 2. Pour les positions GPS, on fait une requÃªte SQL ultra-lÃ©gÃ¨re
        # DISTINCT ON : pour n'avoir qu'une seule ligne par compteur
        query_loc = "SELECT DISTINCT ON (counter_id) counter_id, lat, lon FROM velo_clean"
        
        with db.engine.connect() as conn:
            df_locs = pd.read_sql(query_loc, conn)
            
        # 3. Fusion (Join) en mÃ©moire (rapide car les DataFrames sont maintenant petits)
        df_merged = pd.merge(df_preds, df_locs, on="counter_id", how="left")
        
        # Nettoyage
        df_merged = df_merged.rename(columns={
            'predicted_values': 'predicted_intensity',
            'datetime': 'date'
        })
        df_merged['temperature_2m'] = 15.0 
        
        # Conversion date pour JSON
        if 'date' in df_merged.columns:
            df_merged['date'] = df_merged['date'].astype(str)

        return df_merged.to_dict(orient="records")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    

@app.get("/map-data")
def get_map_data():
    try:
        # 1. DÃ‰FINITION DE LA FENÃŠTRE DE TEMPS
        # On regarde : Hier (pour le contexte), Aujourd'hui et Demain (J+1)
        # Cela permet de voir oÃ¹ s'arrÃªtent les donnÃ©es rÃ©elles.
        
        # --- REQUÃŠTE A : DONNÃ‰ES RÃ‰ELLES (La VÃ©ritÃ©) ---
        query_real = """
            SELECT counter_id, datetime, intensity as real_count
            FROM velo_clean
            WHERE datetime >= CURRENT_DATE - INTERVAL '1 day'
        """
        
        # --- REQUÃŠTE B : PRÃ‰DICTIONS (Le Futur) ---
        query_pred = """
            SELECT counter_id, datetime, predicted_values as pred_count
            FROM model_data
            WHERE datetime >= CURRENT_DATE - INTERVAL '1 day' 
            AND datetime < CURRENT_DATE + INTERVAL '2 day'
        """

        # --- REQUÃŠTE C : COORDONNÃ‰ES (Pour la carte) ---
        query_loc = "SELECT DISTINCT ON (counter_id) counter_id, lat, lon FROM velo_clean"

        # ExÃ©cution optimisÃ©e (3 requÃªtes lÃ©gÃ¨res)
        with db.engine.connect() as conn:
            df_real = pd.read_sql(query_real, conn)
            df_pred = pd.read_sql(query_pred, conn)
            df_locs = pd.read_sql(query_loc, conn)

        # 2. STANDARDISATION DES DATES
        # Pour que la fusion fonctionne, il faut Ãªtre sÃ»r que les formats soient identiques
        df_real['datetime'] = pd.to_datetime(df_real['datetime'])
        df_pred['datetime'] = pd.to_datetime(df_pred['datetime'])

        # 3. FUSION INTELLIGENTE (Outer Join)
        # On assemble RÃ©el et PrÃ©dictions sur (counter_id + datetime)
        df_merged = pd.merge(df_pred, df_real, on=['counter_id', 'datetime'], how='outer')

        # 4. LA LOGIQUE "COMBLE LE RESTE" (Coalesce)
        # C'est ici que la magie opÃ¨re :
        # "Prends la valeur rÃ©elle. Si elle est vide (NaN), prends la prÃ©diction."
        df_merged['final_count'] = df_merged['real_count'].fillna(df_merged['pred_count'])
        
        # On remplit les derniers trous (cas oÃ¹ ni rÃ©el ni prÃ©diction n'existent) par 0
        df_merged['final_count'] = df_merged['final_count'].fillna(0)

        # On ajoute un flag pour le frontend (pour savoir si c'est du rÃ©el ou du prÃ©dictif)
        # Si real_count n'est pas nul, c'est "History", sinon "Prediction"
        df_merged['status'] = df_merged['real_count'].notna().map({True: 'Reel', False: 'Prediction'})

        # 5. AJOUT DES COORDONNÃ‰ES
        df_final = pd.merge(df_merged, df_locs, on="counter_id", how="left")

        # 6. NETTOYAGE FINAL
        # On ne garde que les colonnes utiles
        df_final = df_final[['counter_id', 'datetime', 'final_count', 'status', 'lat', 'lon', 'real_count']]
        
        df_final = df_final.rename(columns={
            'final_count': 'predicted_intensity', # On garde ce nom pour compatibilitÃ© frontend
            'datetime': 'date'
        })
        
        # Ajout mÃ©tÃ©o par dÃ©faut
        df_final['temperature_2m'] = 15.0
        # Conversion date pour JSON
        df_final['date'] = df_final['date'].astype(str)
        
        # Tri par date pour avoir une belle courbe
        df_final = df_final.sort_values(by=['counter_id', 'date'])

        return df_final.to_dict(orient="records")

    except Exception as e:
        print(f"âŒ Erreur map-data: {e}")
        raise HTTPException(status_code=500, detail=str(e))