from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import os
import pandas as pd
from dotenv import load_dotenv
from backend.data.schemas import Database
from functools import lru_cache

# 1. Configuration
load_dotenv()
app = FastAPI(title="VÃ©loMag API")

# Autoriser Streamlit (qui tourne sur un autre port) Ã  parler Ã  l'API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- GESTION DE LA BDD (Lazy Loading) ---
@lru_cache()
def get_db():
    """
    CrÃ©e la connexion BDD une seule fois et la garde en cache.
    Ã‰vite de bloquer le dÃ©marrage de l'API si la BDD dort.
    """
    try:
        USER = os.getenv("user")
        PASSWORD = os.getenv("password")
        HOST = os.getenv("host")
        PORT = os.getenv("port")
        DBNAME = os.getenv("dbname")
        
        # sslmode=require est souvent obligatoire sur Azure
        db_url = f"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DBNAME}?sslmode=require"
        return Database(db_url)
    except Exception as e:
        print(f" Erreur Config BDD: {e}")
        return None

@app.get("/")
def root():
    return {"message": "API VÃ©loMag est en ligne ! ðŸš²", "status": "secure & fast"}

# #Version lourde :
# @app.get("/counters")
# def get_list_counters():
#     """Retourne la liste unique des compteurs disponibles."""
#     try:
#         # On lit la table model_data car c'est elle qui contient les prÃ©dictions prÃªtes
#         df = db.pull_data("model_data")
#         if df.empty:
#             return {"counters": []}
        
#         unique_ids = df['counter_id'].unique().tolist()
#         return {"counters": unique_ids}
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))

@app.get("/counters")
def get_list_counters():
    """Retourne la liste unique des compteurs (OptimisÃ© SQL)."""
    db = get_db()
    if not db: raise HTTPException(500, "Database non connectÃ©e")
    
    try:
        # OPTIMISATION : On ne rÃ©cupÃ¨re que les noms uniques
        query = "SELECT DISTINCT counter_id FROM model_data ORDER BY counter_id"
        
        with db.engine.connect() as conn:
            df = pd.read_sql(query, conn)
        
        if df.empty: return {"counters": []}
        
        return {"counters": df['counter_id'].tolist()}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
# Cette version est trop lourde :C'est trÃ¨s clair. Oublions le monitoring.

# Voici l'explication technique : le problÃ¨me principal est la mÃ©thode "Tout tÃ©lÃ©charger, puis filtrer". Actuellement, quand un utilisateur demande l'historique d'un seul compteur, voici ce qui se passe :

# - Python appelle la Base de DonnÃ©es (PostgreSQL) et PostgreSQL envoie TOUTE la table velo_clean (800 000+ lignes, peut-Ãªtre 100 Mo ou plus) via le rÃ©seau vers Azure Web App.

# - Pandas charge ces 100 Mo dans la RAM, ensuite Pandas filtre pour ne garder que les 5 000 lignes du compteur demandÃ©.

# - Python jette les 795 000 autres lignes Ã  la poubelle.

# @app.get("/history/{counter_id}")
# def get_history(counter_id: str):
#     """Retourne l'historique rÃ©el pour un compteur."""
#     try:
#         df = db.pull_data("velo_clean")
#         # Filtrage
#         df = df[df['counter_id'] == counter_id]
#         df = df[['datetime', 'intensity']]         # on ne renvoie que les colonnes utiles
#         df = df.rename(columns={'intensity': 'count'})
#         # Conversion JSON
#         return df.to_dict(orient="records")
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))

@app.get("/history/{counter_id}")
def get_history(counter_id: str):
    """Retourne l'historique rÃ©el (SÃ©curisÃ© & OptimisÃ©)."""
    db = get_db()
    if not db: raise HTTPException(500, "Database non connectÃ©e")
    
    try:
        # SÃ‰CURITÃ‰ : On utilise un placeholder %(id)s
        # OPTIMISATION : On ne sÃ©lectionne que les colonnes utiles
        query = """
            SELECT datetime, intensity 
            FROM velo_clean 
            WHERE counter_id = %(id)s
            ORDER BY datetime ASC
        """
        
        with db.engine.connect() as conn:
            # L'injection SQL est bloquÃ©e ici grÃ¢ce Ã  'params'
            df = pd.read_sql(query, conn, params={"id": counter_id})
            
        df = df.rename(columns={'intensity': 'count'})
        df['datetime'] = df['datetime'].astype(str)
        
        return df.to_dict(orient="records")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# # Version lourde
# @app.get("/prediction/{counter_id}")
# def get_prediction(counter_id: str):
#     """Retourne les prÃ©dictions (J+1) pour un compteur."""
#     try:
#         df = db.pull_data("model_data")
#         df = df[df['counter_id'] == counter_id]
#         df = df[['datetime', 'predicted_values']]
#         df = df.rename(columns={'predicted_values': 'count'})
#         return df.to_dict(orient="records")
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))

@app.get("/prediction/{counter_id}")
def get_prediction(counter_id: str):
    """Retourne les prÃ©dictions futures (SÃ©curisÃ© & OptimisÃ©)."""
    db = get_db()
    if not db: raise HTTPException(500, "Database non connectÃ©e")

    try:
        # OPTIMISATION : On ne prend que le futur (>= CURRENT_DATE)
        query = """
            SELECT datetime, predicted_values 
            FROM model_data 
            WHERE counter_id = %(id)s
            AND datetime >= CURRENT_DATE
            ORDER BY datetime ASC
        """
        
        with db.engine.connect() as conn:
            df = pd.read_sql(query, conn, params={"id": counter_id})
            
        df = df.rename(columns={'predicted_values': 'count'})
        df['datetime'] = df['datetime'].astype(str)
        
        return df.to_dict(orient="records")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Cette version est lourde aussi : on rÃ©cupÃ¨res aussi tout model_data (lourd) et tout velo_clean pour avoir les positions GPS.
# @app.get("/map-data")
# def get_map_data():
#     """
#     Route spÃ©ciale pour la carte : 
#     RÃ©cupÃ¨re toutes les prÃ©dictions et joint les coordonnÃ©es GPS.
#     """
#     try:
#         # 1. RÃ©cupÃ©rer toutes les prÃ©dictions
#         df_preds = db.pull_data("model_data")
        
#         # 2. RÃ©cupÃ©rer les positions GPS (depuis velo_clean ou une table counters)
#         # On fait une requÃªte SQL pour avoir une seule ligne par compteur avec lat/lon
#         query = "SELECT DISTINCT ON (counter_id) counter_id, lat, lon FROM velo_clean"
#         with db.engine.connect() as conn:
#             df_locs = pd.read_sql(query, conn)
            
#         # 3. Fusionner (Join)
#         # On ajoute lat/lon aux prÃ©dictions
#         df_merged = pd.merge(df_preds, df_locs, on="counter_id", how="left")
        
#         # 4. Nettoyage pour le frontend
#         # On renomme pour coller Ã  votre code frontend
#         df_merged = df_merged.rename(columns={
#             'predicted_values': 'predicted_intensity',
#             'datetime': 'date'
#         })
        
#         # Astuce : Comme on n'a pas stockÃ© la tempÃ©rature dans model_data, 
#         # on met une valeur par dÃ©faut pour ne pas faire planter votre interface.
#         df_merged['temperature_2m'] = 15.0 
        
#         return df_merged.to_dict(orient="records")
        
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))


# #premiÃ¨re version optimisÃ©
# @app.get("/map-data")
# def get_map_data():
#     try:
#         # 1. On rÃ©cupÃ¨re les prÃ©dictions (J+1 seulement)
#         # Supposons que tu veux les prÃ©dictions de demain.
#         # Si model_data ne contient QUE les prÃ©dictions futures, pull_data est OK.
#         # Sinon, il faut filtrer par date en SQL.
#         df_preds = db.pull_data("model_data")

#         # 2. Pour les positions GPS, on fait une requÃªte SQL ultra-lÃ©gÃ¨re
#         # DISTINCT ON : pour n'avoir qu'une seule ligne par compteur
#         query_loc = "SELECT DISTINCT ON (counter_id) counter_id, lat, lon FROM velo_clean"
        
#         with db.engine.connect() as conn:
#             df_locs = pd.read_sql(query_loc, conn)
            
#         # 3. Fusion (Join) en mÃ©moire (rapide car les DataFrames sont maintenant petits)
#         df_merged = pd.merge(df_preds, df_locs, on="counter_id", how="left")
        
#         # Nettoyage
#         df_merged = df_merged.rename(columns={
#             'predicted_values': 'predicted_intensity',
#             'datetime': 'date'
#         })
#         df_merged['temperature_2m'] = 15.0 
        
#         # Conversion date pour JSON
#         if 'date' in df_merged.columns:
#             df_merged['date'] = df_merged['date'].astype(str)

#         return df_merged.to_dict(orient="records")
        
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))
    
# #deuxieme version
# @app.get("/map-data")
# def get_map_data():
#     try:
#         # 1. DÃ‰FINITION DE LA FENÃŠTRE DE TEMPS
#         # On regarde : Hier (pour le contexte), Aujourd'hui et Demain (J+1)
#         # Cela permet de voir oÃ¹ s'arrÃªtent les donnÃ©es rÃ©elles.
        
#         # --- REQUÃŠTE A : DONNÃ‰ES RÃ‰ELLES (La VÃ©ritÃ©) ---
#         query_real = """
#             SELECT counter_id, datetime, intensity as real_count
#             FROM velo_clean
#             WHERE datetime >= CURRENT_DATE - INTERVAL '1 day'
#         """
        
#         # --- REQUÃŠTE B : PRÃ‰DICTIONS (Le Futur) ---
#         query_pred = """
#             SELECT counter_id, datetime, predicted_values as pred_count
#             FROM model_data
#             WHERE datetime >= CURRENT_DATE - INTERVAL '1 day' 
#             AND datetime < CURRENT_DATE + INTERVAL '2 day'
#         """

#         # --- REQUÃŠTE C : COORDONNÃ‰ES (Pour la carte) ---
#         query_loc = "SELECT DISTINCT ON (counter_id) counter_id, lat, lon FROM velo_clean"

#         # ExÃ©cution optimisÃ©e (3 requÃªtes lÃ©gÃ¨res)
#         with db.engine.connect() as conn:
#             df_real = pd.read_sql(query_real, conn)
#             df_pred = pd.read_sql(query_pred, conn)
#             df_locs = pd.read_sql(query_loc, conn)

#         # 2. STANDARDISATION DES DATES
#         # Pour que la fusion fonctionne, il faut Ãªtre sÃ»r que les formats soient identiques
#         df_real['datetime'] = pd.to_datetime(df_real['datetime'])
#         df_pred['datetime'] = pd.to_datetime(df_pred['datetime'])

#         # 3. FUSION INTELLIGENTE (Outer Join)
#         # On assemble RÃ©el et PrÃ©dictions sur (counter_id + datetime)
#         df_merged = pd.merge(df_pred, df_real, on=['counter_id', 'datetime'], how='outer')

#         # 4. LA LOGIQUE "COMBLE LE RESTE" (Coalesce)
#         # C'est ici que la magie opÃ¨re :
#         # "Prends la valeur rÃ©elle. Si elle est vide (NaN), prends la prÃ©diction."
#         df_merged['final_count'] = df_merged['real_count'].fillna(df_merged['pred_count'])
        
#         # On remplit les derniers trous (cas oÃ¹ ni rÃ©el ni prÃ©diction n'existent) par 0
#         df_merged['final_count'] = df_merged['final_count'].fillna(0)

#         # On ajoute un flag pour le frontend (pour savoir si c'est du rÃ©el ou du prÃ©dictif)
#         # Si real_count n'est pas nul, c'est "History", sinon "Prediction"
#         df_merged['status'] = df_merged['real_count'].notna().map({True: 'Reel', False: 'Prediction'})

#         # 5. AJOUT DES COORDONNÃ‰ES
#         df_final = pd.merge(df_merged, df_locs, on="counter_id", how="left")

#         # 6. NETTOYAGE FINAL
#         # On ne garde que les colonnes utiles
#         df_final = df_final[['counter_id', 'datetime', 'final_count', 'status', 'lat', 'lon', 'real_count']]
        
#         df_final = df_final.rename(columns={
#             'final_count': 'predicted_intensity', # On garde ce nom pour compatibilitÃ© frontend
#             'datetime': 'date'
#         })
        
#         # Ajout mÃ©tÃ©o par dÃ©faut
#         df_final['temperature_2m'] = 15.0
#         # Conversion date pour JSON
#         df_final['date'] = df_final['date'].astype(str)
        
#         # Tri par date pour avoir une belle courbe
#         df_final = df_final.sort_values(by=['counter_id', 'date'])

#         return df_final.to_dict(orient="records")

#     except Exception as e:
#         print(f" Erreur map-data: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

@app.get("/map-data")
def get_map_data():
    """
    Route Carte : Assure une continuitÃ© parfaite Historique -> PrÃ©diction.
    On rÃ©cupÃ¨re large en SQL pour Ãªtre sÃ»r d'avoir une prÃ©diction en face de chaque trou potentiel.
    """
    db = get_db()
    if not db: raise HTTPException(500, "Database non connectÃ©e")

    try:
        # 1. DÃ‰FINITION DE LA FENÃŠTRE LARGE
        # On regarde 3 jours en arriÃ¨re pour Ãªtre sÃ»r de combler les trous rÃ©cents
        # et 2 jours en avant pour le futur.
        
        # --- REQUÃŠTE A : DONNÃ‰ES RÃ‰ELLES ---
        query_real = """
            SELECT counter_id, datetime, intensity as real_count
            FROM velo_clean
            WHERE datetime >= CURRENT_DATE - INTERVAL '3 day'
        """
        
        # --- REQUÃŠTE B : PRÃ‰DICTIONS (CORRECTION ICI) ---
        # AVANT : datetime >= CURRENT_DATE (Trop strict, crÃ©ait des trous)
        # APRÃˆS : datetime >= CURRENT_DATE - INTERVAL '3 day'
        # On rÃ©cupÃ¨re les prÃ©dictions sur la MÃŠME pÃ©riode que le rÃ©el.
        query_pred = """
            SELECT counter_id, datetime, predicted_values as pred_count
            FROM model_data
            WHERE datetime >= CURRENT_DATE - INTERVAL '3 day' 
            AND datetime < CURRENT_DATE + INTERVAL '2 day'
        """

        query_loc = "SELECT DISTINCT ON (counter_id) counter_id, lat, lon FROM velo_clean"

        with db.engine.connect() as conn:
            df_real = pd.read_sql(query_real, conn)
            df_pred = pd.read_sql(query_pred, conn)
            df_locs = pd.read_sql(query_loc, conn)

        # 2. STANDARDISATION
        df_real['datetime'] = pd.to_datetime(df_real['datetime'])
        df_pred['datetime'] = pd.to_datetime(df_pred['datetime'])

        # 3. FUSION (OUTER JOIN)
        # Cela crÃ©e une table avec toutes les dates possibles.
        # Si une date existe dans Pred mais pas dans Real (le trou), une ligne est crÃ©Ã©e.
        df_merged = pd.merge(df_pred, df_real, on=['counter_id', 'datetime'], how='outer')

        # 4. LE "ZIPPAGE" (Comblement des trous)
        # Logique : "Si j'ai une valeur rÃ©elle, je la prends. SINON, je prends la prÃ©diction."
        # C'est grÃ¢ce Ã  Ã§a que tes lags servent : la prÃ©diction est lÃ  pour boucher le trou.
        df_merged['final_count'] = df_merged['real_count'].fillna(df_merged['pred_count'])
        
        # SÃ©curitÃ© finale : s'il n'y a ni l'un ni l'autre, on met 0
        df_merged['final_count'] = df_merged['final_count'].fillna(0)

        # 5. AJOUT COORDONNÃ‰ES
        df_final = pd.merge(df_merged, df_locs, on="counter_id", how="left")

        # 6. FORMATAGE
        df_final = df_final.rename(columns={
            'final_count': 'predicted_intensity',
            'datetime': 'date'
        })
        
        df_final['temperature_2m'] = 15.0
        df_final['date'] = df_final['date'].astype(str)
        
        # On trie pour avoir une belle ligne continue
        df_final = df_final.sort_values(by=['counter_id', 'date'])

        # Nettoyage des colonnes inutiles pour le JSON
        cols_to_keep = ['counter_id', 'date', 'predicted_intensity', 'lat', 'lon', 'temperature_2m']
        cols_final = [c for c in cols_to_keep if c in df_final.columns]

        return df_final[cols_final].to_dict(orient="records")

    except Exception as e:
        print(f" Erreur map-data: {e}")
        raise HTTPException(status_code=500, detail=str(e))